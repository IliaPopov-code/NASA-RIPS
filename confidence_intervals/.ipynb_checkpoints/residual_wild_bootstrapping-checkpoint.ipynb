{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8b5970-a71c-4b25-9b47-1aebb8f2b96a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2fd51-9a37-4e83-9834-cf68053e502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b66c0-abdd-456b-8aff-ee22114f2ea1",
   "metadata": {},
   "source": [
    "# Make Intervals (Functions)\n",
    "\n",
    "In this section, we provide methods to create the appropriate confidence intervals. This means, that we will provide a list of lower bounds and a list of upper bounds where index $i$ in each list is a lower bound and upper bound pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e694e21-edbf-4cd0-b9ad-d32d21f90830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid_bootstrap(y_pred, residuals, n_bs=1000):\n",
    "    \"\"\"\n",
    "    Perform residual bootstrapping to generate confidence intervals for model predictions.\n",
    "\n",
    "    This function creates bootstrapped samples of residuals, adds them to the predicted values, \n",
    "    and calculates the lower and upper bounds of a 95% confidence interval for the predictions. \n",
    "\n",
    "\n",
    "    Inputs:\n",
    "        y_pred (array-like): The predicted values from a model to which the bootstrapped \n",
    "            residuals are added.\n",
    "        residuals (array-like): The residuals (observed - predicted values), which are \n",
    "            randomly sampled and added to `y_pred` to generate new bootstrapped predictions.\n",
    "        n_bs (int): The number of bootstrap samples to generate. Default is 1000.\n",
    "\n",
    "    Outputs:\n",
    "        (numpy.ndarray): The lower bounds of the 95% confidence interval for each prediction, \n",
    "            calculated at the 2.5th percentile of the bootstrapped predictions.\n",
    "        (numpy.ndarray): The upper bounds of the 95% confidence interval for each prediction, \n",
    "            calculated at the 97.5th percentile of the bootstrapped predictions.\n",
    "    \"\"\"\n",
    "    bootstrap_predictions = []\n",
    "    for i in range(n_bs):\n",
    "        bootstrap_sample = np.random.choice(residuals, size=len(residuals))\n",
    "        bootstrap_pred = y_pred + bootstrap_sample\n",
    "        bootstrap_predictions.append(bootstrap_pred)\n",
    "\n",
    "    bootstrap_arr = np.array(bootstrap_predictions)\n",
    "    lower_bounds = np.percentile(bootstrap_arr, 2.5, axis=0)\n",
    "    upper_bounds = np.percentile(bootstrap_arr, 97.5, axis=0)\n",
    "\n",
    "    return lower_bounds, upper_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e36dd-698c-4c4f-848c-a3f8e8d2e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wild_bootstrap(y_pred, residuals, n_bs=1000):\n",
    "    \"\"\"\n",
    "    Perform wild bootstrapping to generate confidence intervals for model predictions.\n",
    "\n",
    "    This function creates bootstrapped samples of residuals, adds them to the predicted values, \n",
    "    and calculates the lower and upper bounds of a 95% confidence interval for the predictions. \n",
    "\n",
    "\n",
    "    Inputs:\n",
    "        y_pred (array-like): The predicted values from a model to which the bootstrapped \n",
    "            residuals are added.\n",
    "        residuals (array-like): The residuals (observed - predicted values), which are \n",
    "            randomly sampled and added to `y_pred` to generate new bootstrapped predictions.\n",
    "        n_bs (int): The number of bootstrap samples to generate. Default is 1000.\n",
    "\n",
    "    Outputs:\n",
    "        (numpy.ndarray): The lower bounds of the 95% confidence interval for each prediction, \n",
    "            calculated at the 2.5th percentile of the bootstrapped predictions.\n",
    "        (numpy.ndarray): The upper bounds of the 95% confidence interval for each prediction, \n",
    "            calculated at the 97.5th percentile of the bootstrapped predictions.\n",
    "    \"\"\"\n",
    "    bootstrap_predictions = []\n",
    "    \n",
    "    for i in range(n_bs):\n",
    "        bootstrap_V_i = np.random.normal(0, 1, size=len(residuals))\n",
    "        bootstrap_pred = y_pred + bootstrap_V_i * residuals\n",
    "        bootstrap_predictions.append(bootstrap_pred)\n",
    "        \n",
    "    bootstrap_arr = np.array(bootstrap_predictions)\n",
    "    lower_bounds = np.percentile(bootstrap_arr, 2.5, axis=0)\n",
    "    upper_bounds = np.percentile(bootstrap_arr, 97.5, axis=0)\n",
    "\n",
    "    return lower_bounds, upper_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325c682-ecea-4e29-a722-2c5a266dac0f",
   "metadata": {},
   "source": [
    "# Plot Intervals (Functions)\n",
    "\n",
    "Provides code that plots the functions given predictions, residuals, and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb3dd8-d428-4886-9f53-4c85bf2c2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confidence_interval_plots(predictions, residuals, observations, data_label=\"\", file_name=None, normalize=False, y_limit=None):\n",
    "    \"\"\"\n",
    "    Generate and plot confidence intervals using both residual and wild bootstrapping methods, \n",
    "    and calculate coverage rates for the predictions.\n",
    "\n",
    "    Inputs:\n",
    "        predictions (numpy.ndarray): The predicted values from the model.\n",
    "        residuals (numpy.ndarray): The residuals (observed - predicted values) used for bootstrapping.\n",
    "        observations (numpy.ndarray): The true observed values used for sorting and coverage rate calculation.\n",
    "        data_label (str): A label for the plot title (default is \"\").\n",
    "        file_name (str): If provided, the plot will be saved with this filename (default is None).\n",
    "        normalize (bool): If True, predictions and confidence intervals will be normalized by the observations (default is False).\n",
    "        y_limit (float): The y-axis limit for the plots (default is None).\n",
    "\n",
    "    Outputs:\n",
    "        None: The function plots the confidence intervals and prints the coverage rate \n",
    "              for both residual and wild bootstrapping methods.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot settings\n",
    "    pred_color = \"green\"\n",
    "    obs_color = \"magenta\"\n",
    "    conf_color = \"skyblue\"\n",
    "    linewidth = 4\n",
    "    title_size = 18\n",
    "    axis_size = 17\n",
    "    legend_size = 16\n",
    "    tick_size = 15\n",
    "    \n",
    "    # Generate bootstrap confidence intervals\n",
    "    lower_resid_ci, upper_resid_ci = resid_bootstrap(predictions, residuals)\n",
    "    lower_wild_ci, upper_wild_ci = wild_bootstrap(predictions, residuals)\n",
    "\n",
    "    # Sort by observations for a clearer plot\n",
    "    sorted_indices = np.argsort(observations)\n",
    "    sorted_observations = observations[sorted_indices]\n",
    "    sorted_pred = predictions[sorted_indices]\n",
    "    sorted_lower_resid_ci = lower_resid_ci[sorted_indices]\n",
    "    sorted_upper_resid_ci = upper_resid_ci[sorted_indices]\n",
    "\n",
    "    # Normalize data if needed\n",
    "    if normalize:\n",
    "        sorted_pred = sorted_pred / sorted_observations\n",
    "        sorted_lower_resid_ci = sorted_lower_resid_ci / sorted_observations\n",
    "        sorted_upper_resid_ci = sorted_upper_resid_ci / sorted_observations\n",
    "        sorted_observations = np.ones_like(sorted_observations)\n",
    "\n",
    "    # Create the x-axis values\n",
    "    x_values = np.arange(len(sorted_observations))\n",
    "\n",
    "    # Plot Residual Bootstrap CI\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(x_values, sorted_pred, label='Predicted', color=pred_color)\n",
    "    plt.plot(x_values, sorted_observations, label='Observed', color=obs_color, linewidth=linewidth)\n",
    "    \n",
    "    if y_limit:\n",
    "        plt.ylim(-5, y_limit)\n",
    "        \n",
    "    plt.fill_between(x_values, sorted_lower_resid_ci, sorted_upper_resid_ci, color=conf_color, alpha=1, label='RCI')\n",
    "    plt.xlabel(r'Observations (sorted by increasing $\\sigma_W$)', fontsize=axis_size)\n",
    "    plt.ylabel('Observation Values (m/s)', fontsize=axis_size)\n",
    "    plt.title(data_label, fontsize=title_size)\n",
    "    plt.legend(fontsize=legend_size)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=tick_size)\n",
    "\n",
    "    if file_name:\n",
    "        plt.savefig(f\"rci_{file_name}.png\", dpi=300)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Calculate coverage rate for Residual Bootstrap CI\n",
    "    within_ci = (sorted_observations >= sorted_lower_resid_ci) & (sorted_observations <= sorted_upper_resid_ci)\n",
    "    coverage_rate_resid = np.sum(within_ci) / len(sorted_observations)\n",
    "    print(f\"Coverage rate for RCI: {coverage_rate_resid:.2%}\")\n",
    "\n",
    "    # Reset data (from normalization) and sort Wild Bootstrap CI\n",
    "    sorted_pred = predictions[sorted_indices]\n",
    "    sorted_observations = observations[sorted_indices]\n",
    "    sorted_lower_wild_ci = lower_wild_ci[sorted_indices]\n",
    "    sorted_upper_wild_ci = upper_wild_ci[sorted_indices]\n",
    "\n",
    "    # Normalize data if needed\n",
    "    if normalize:\n",
    "        sorted_pred = sorted_pred / sorted_observations\n",
    "        sorted_lower_wild_ci = sorted_lower_wild_ci / sorted_observations\n",
    "        sorted_upper_wild_ci = sorted_upper_wild_ci / sorted_observations\n",
    "        sorted_observations = np.ones_like(sorted_observations)\n",
    "\n",
    "    # Plot Wild CI\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(x_values, sorted_pred, label='Predicted', color=pred_color)\n",
    "    plt.plot(x_values, sorted_observations, label='Observed', color=obs_color, linewidth=linewidth)\n",
    "    \n",
    "    if y_limit:\n",
    "        plt.ylim(-5, y_limit)\n",
    "\n",
    "    plt.fill_between(x_values, sorted_lower_wild_ci, sorted_upper_wild_ci, color=conf_color, alpha=1, label='Confidence Interval')\n",
    "    plt.xlabel(r'Observations (sorted by increasing $\\sigma_W$)', fontsize=axis_size)\n",
    "    plt.ylabel('Observation Values (m/s)', fontsize=axis_size)\n",
    "    plt.title(data_label, fontsize=title_size)\n",
    "    plt.legend(fontsize=legend_size)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=tick_size)\n",
    "    \n",
    "    if file_name:\n",
    "        plt.savefig(f\"wci_{file_name}.png\", dpi=300)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    # Calculate coverage rate for Wild Bootstrap CI\n",
    "    within_ci = (sorted_observations >= sorted_lower_wild_ci) & (sorted_observations <= sorted_upper_wild_ci)\n",
    "    coverage_rate_wild = np.sum(within_ci) / len(sorted_observations)\n",
    "    print(f\"Coverage rate for WCI: {coverage_rate_wild:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed156aa0-71df-4360-94bc-e56dd4eb680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depends on the variables defined in the box above\n",
    "def get_site_vars(site_val, lev):\n",
    "    '''\n",
    "    Retrieves site-specific vertical wind velocity data and model predictions at a\n",
    "    given atmospheric level and calculates teh residuals between observations/ground\n",
    "    truth and predictions for the selected site.\n",
    "\n",
    "    Inputs:\n",
    "        site_val (int): Index of the site in the predefined list of sites. The \n",
    "            valid indices are: 0: \"asi\", 1: \"cor\", 2: \"nsa\", 3: \"sgp_cirrus\", 4: \n",
    "            \"sgp_pbl\".\n",
    "        lev (int): The vertical level to retrieve data from. This is 1-based and is \n",
    "            used to select level in the data for both observations and predictions.\n",
    "\n",
    "    Outputs:\n",
    "        site_resid (numpy.ndarray): Residuals between the observed data and the Wnet \n",
    "            predictions for the specified site and level\n",
    "        site_resid_prior (numpy.ndarray): Residuals between the osberved data and the \n",
    "            Wnet-prior predictions for the specified site and level\n",
    "        site_wnet (numpy.ndarray): The predicted sigma_W values from Wnet\n",
    "        site_wnet_prior (numpy.ndarray): The predicted sigma_W values from Wnet-prior\n",
    "        site_observed (numpy.ndarray): The observed sigma_W values for the specified \n",
    "            site and level.\n",
    "\n",
    "    '''\n",
    "    # retrieve observations by site and level\n",
    "    sites = [\"asi\", \"cor\", \"nsa\", \"sgp_cirrus\", \"sgp_pbl\"]\n",
    "    site = sites[site_val]\n",
    "\n",
    "    # Retrieve predictions and reshape\n",
    "    X, obs = get_data(site)\n",
    "    X = X.sel(lev = lev - 1)\n",
    "    heights = obs.coords['height'].values\n",
    "    obs = obs.sel(height = heights[lev - 1])\n",
    "    \n",
    "    wnet_prior_preds = wnet_prior.predict(X, batch_size=2048)\n",
    "    wnet_preds = wnet.predict(X, batch_size=2048)\n",
    "    wnet_predictions = wnet_preds.reshape(-1)\n",
    "    wnet_prior_predictions = wnet_prior_preds.reshape(-1)\n",
    "\n",
    "    # drop NA\n",
    "    mask = obs != 0\n",
    "    obs_filtered = obs[mask]\n",
    "\n",
    "    # filter and assign site values\n",
    "    site_observed = obs_filtered.values\n",
    "    site_wnet = wnet_predictions[mask]\n",
    "    site_wnet_prior = wnet_prior_predictions[mask]\n",
    "\n",
    "    # calculate residuals\n",
    "    site_resid = site_observed - site_wnet\n",
    "    site_resid_prior = site_observed - site_wnet_prior\n",
    "\n",
    "    return site_resid, site_resid_prior, site_wnet, site_wnet_prior, site_observed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea6fe5-eaaf-4699-acef-a6defbacc005",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "\n",
    "Since G5NR data is provided in Xarray files, we preprocess the data before performing inference. This section provides the correct functions to process data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcee183-231e-4dca-a87a-dbfc5d892a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in MERRA and ASR data the same way as PDF_bysite.py\n",
    "def outlier(x):\n",
    "    '''\n",
    "    Calculates the standardized anomaly (z-score) of the input data along the 'time' dimension. \n",
    "    Returns the absolute value of the z-score to detect outliers.\n",
    "\n",
    "    Inputs:\n",
    "        x (xarray.DataArray): Input data with a 'time' dimension.\n",
    "\n",
    "    Outputs:\n",
    "        xarray.DataArray: Absolute value of the z-score of the input data, used for outlier detection.\n",
    "    '''\n",
    "    return abs((x-x.mean(dim='time')) / x.std(dim='time')) # causes some RuntimeWarnings (division by 0/nan)\n",
    "\n",
    "\n",
    "def standardize(ds):\n",
    "    '''\n",
    "    Standardizes the input dataset based on predefined means and standard deviations for specific variables.\n",
    "\n",
    "    Inputs:\n",
    "        ds (xarray.Dataset): Input dataset containing variables to be standardized.\n",
    "\n",
    "    Outputs:\n",
    "        xarray.Dataset: Standardized dataset where each variable is transformed using precomputed mean and \n",
    "            standard deviation.\n",
    "    '''\n",
    "    i = 0\n",
    "    m = [243.9, 0.6, 6.3, 0.013, 0.0002, 5.04, 21.8, 0.002, 9.75e-7, 7.87e-6]  #hardcoded from G5NR\n",
    "    s = [30.3, 0.42, 16.1, 7.9, 0.05, 20.6, 20.8, 0.0036, 7.09e-6, 2.7e-5]\n",
    "    \n",
    "    for v in  ds.data_vars:\n",
    "        ds[v] = (ds[v] - m[i])/s[i]\n",
    "        i = i+1\n",
    "    return ds\n",
    "    \n",
    "# sites = asi, cor, nsa, sgp_cirrus, sgp_pbl\n",
    "def get_data(site='', chunk_size=512*72):\n",
    "    '''\n",
    "    Loads and processes MERRA and ASR datasets for the specified site, performs standardization, \n",
    "    outlier removal, and data alignment between observations and model inputs.\n",
    "\n",
    "    Inputs:\n",
    "        site (str): The site identifier for which data is to be retrieved (e.g., 'asi', 'cor'). Default is an empty string.\n",
    "        chunk_size (int): The chunk size to be used for loading the data. Default is 512*72.\n",
    "\n",
    "    Outputs:\n",
    "        X (xarray.DataArray): Standardized and aligned model input data, reshaped and stacked into a 2D array for the model.\n",
    "        y (xarray.DataArray): Target data (observed vertical wind velocity standard deviation) aligned with the input data.\n",
    "    '''\n",
    "    file_path = \"\" # Insert file_path location\n",
    "    path_asr = f'{file_path}/Wstd_asr_resampled_stdev30min_72lv_{site}.nc' \n",
    "    path_merra = f\"{file_path}/Merra_input_asr_72lv_{site}.nc\"\n",
    "\n",
    "    data_obs = xr.open_mfdataset(path_asr, parallel=True)\n",
    "    data_merra = xr.open_mfdataset(path_merra, parallel=True, chunks={\"time\": 2560})\n",
    "\n",
    "    # ================= process obs ====================\n",
    "    data_obs = data_obs.where((data_obs != -9999.) and (data_obs < 15.))\n",
    "    data_std = (data_obs.where(data_obs > 0.001)).groupby('time.month').map(outlier)\n",
    "    data_obs = data_obs.where(data_std < 2.5) \n",
    "    data_obs = data_obs.dropna('time', how='all', thresh=2) \n",
    "    data_obs = data_obs.fillna(0) \n",
    "    \n",
    "    # ================= process merra ==================\n",
    "    data_merra = data_merra.resample(time=\"5min\").interpolate(\"linear\") # calculates SD\n",
    "    data_merra = data_merra[['T', 'AIRD', 'U', 'V', 'W', 'KM', 'RI', 'QV', 'QI', 'QL']] \n",
    "    \n",
    "    # ================= align merra w/ obs ===================\n",
    "    data_merra, data_obs = xr.align(data_merra, data_obs, exclude = {'height', 'lev'}) \n",
    "\n",
    "    # ================= prep model input X (standardize, add 4 surface vars) =================\n",
    "    X = xr.map_blocks(standardize, data_merra, template=data_merra) \n",
    "    \n",
    "    levs = X.coords['lev'].values\n",
    "    num_levs = len(levs) # should be 72\n",
    "    surface_vars = ['AIRD', 'KM', 'RI', 'QV']\n",
    "    for sv in surface_vars:\n",
    "        sv_row = X[sv].sel(lev=[71]).squeeze() \n",
    "        \n",
    "        X_sv = sv_row\n",
    "        for i in range(num_levs - 1):\n",
    "            X_sv = xr.concat([X_sv, sv_row], dim='lev')\n",
    "\n",
    "        X[sv + \"_sfc\"] = X_sv.assign_coords(lev=levs)\n",
    "    \n",
    "    # ==================== clean up input X ========================\n",
    "    X = X.unify_chunks()\n",
    "    X = X.to_array()\n",
    "    X = X.rename({'variable':'feature'}) \n",
    "    X = X.stack(s=('time', 'lev'))\n",
    "    X = X.squeeze() \n",
    "    X = X.transpose()\n",
    "    X = X.chunk({'s': 72*1024}) \n",
    "    \n",
    "    # ==================== clean up target Y =======================\n",
    "    y = data_obs['W_asr_std'] \n",
    "    y = y.stack(s=('time', 'height'))\n",
    "    y = y.chunk({'s': 72*1024})\n",
    "\n",
    "    return X.load(), y.load() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2f4c2-9374-41a6-856f-0998c8008665",
   "metadata": {},
   "source": [
    "# Code to Create Interval Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2ce01-e0d4-43b5-b1a9-a8e70fbc2f7d",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce7513-27ee-457d-a1fc-dc782858ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath_wnet = \"\" # Insert path name to Wnet model\n",
    "modelpath_wnet_prior = \"\" # Insert path name to Wnet-prior\n",
    "wnet = load_model(modelpath_wnet, compile=False)\n",
    "wnet_prior = load_model(modelpath_wnet_prior, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e57341-51f1-4ef9-9852-7dd10353c75b",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9d057-c7fa-43cb-a5eb-9d2e65307288",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_resid, site_resid_prior, site_wnet, site_wnet_prior, site_observed = get_site_vars(4, 72)\n",
    "create_confidence_interval_plots(site_wnet, site_resid, site_observed, \"72\", normalize=True, y_limit = 98)\n",
    "create_confidence_interval_plots(site_wnet_prior, site_resid_prior, site_observed, \"Wnet-prior 72\", normalize=True, y_limit = 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315ca52-a517-44d1-b57b-c9bbb25a301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_resid, site_resid_prior, site_wnet, site_wnet_prior, site_observed = get_site_vars(3, 48)\n",
    "create_confidence_interval_plots(site_wnet, site_resid, site_observed, \"48\", normalize=True)\n",
    "create_confidence_interval_plots(site_wnet_prior, site_resid_prior, site_observed, \"Wnet-prior 72\", normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Goddard)",
   "language": "python",
   "name": "goddard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
