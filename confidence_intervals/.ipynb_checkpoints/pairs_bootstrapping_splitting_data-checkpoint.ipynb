{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "313e4dd7-bfad-4833-8238-243006b6b0e9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e135090-a389-44a4-830c-860619633498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b582853-d353-4de5-af42-25af5ae0ce6d",
   "metadata": {},
   "source": [
    "# Train/val/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4604400-7e8b-459a-9067-5ab0a63cf22f",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The data excpected to be used in this file is data with dimensions lat, lon, lev, and time. \"lat\" and \"lon\", which represents latitude and longitude respectively, are expected to be of resolution 0.5. Furthermore, the expected covariates of the dataset are ['T', 'AIRD', 'U', 'V', 'W', 'KM', \"RI', 'QV', 'QI', 'QL']. The expected \"lev\" values range from 1, 2, ..., 72; each representing an atmospheric level as documented by https://gmao.gsfc.nasa.gov/global_mesoscale/7km-G5NR/docs/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fd4ec-931e-43cf-98bb-674b86bf7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCH_SIZE = 2480\n",
    "\n",
    "US_LOCS = {\"lat1\": 25, \n",
    "           \"lat2\": 50,\n",
    "           \"lon1\": -150,\n",
    "           \"lon2\": -50}\n",
    "COORDS = {\"US\": US_LOCS}\n",
    "\n",
    "MEANS = [243.9, 0.6, 6.3, 0.013, 0.0002, 5.04, 21.8, 0.002, 9.75e-7, 7.87e-6]\n",
    "STDS = [30.3, 0.42, 16.1, 7.9, 0.05, 20.6, 20.8, 0.0036, 7.09e-6, 2.7e-5]\n",
    "SURF_VARS = ['AIRD', 'KM', 'RI', 'QV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e74bf-3f65-480f-afc0-b7f4be9e0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(ds, s, m):\n",
    "    \"\"\"\n",
    "    Standardize the dataset using provided means and standard deviations.\n",
    "    \"\"\"\n",
    "    assert len(list(ds.data_vars)) == len(m)\n",
    "\n",
    "    # data_vars are ['T', 'AIRD', 'U', 'V', 'W', 'KM', \"RI', 'QV', 'QI', 'QL']\n",
    "    for i, var in  enumerate(ds.data_vars):  \n",
    "        ds[var] = (ds[var] - m[i])/s[i]\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcd84c-fcbb-4878-b4ff-c82de1a0e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the data\n",
    "file_path = \"\" # INSERT FILEPATH WITH GLOBAL DATA\n",
    "\n",
    "global_data = xr.open_mfdataset(file_path)\n",
    "global_data = global_data.where(global_data['lev'] != 0, drop=True)\n",
    "\n",
    "# (Optional) Filter coords for quicker processing\n",
    "global_data = global_data.sel(lat=slice(COORDS[\"US\"][\"lat1\"], COORDS[\"US\"][\"lat2\"]), \n",
    "                              lon=slice(COORDS[\"US\"][\"lon1\"], COORDS[\"US\"][\"lon2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46229e-6cc4-41a1-8dea-98860f34b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Covariates and Labels\n",
    "times = [''] # Load desired tiem stamps in the form of a list of strings, ex: ['2006-01-08T10:30:00.000000000']\n",
    "\n",
    "data_in = global_data.sel(time = times)\n",
    "data_in = data_in[['T', 'AIRD', 'U', 'V', 'W', 'KM', 'RI', 'QV', 'QI', 'QL']]\n",
    "data_in = xr.map_blocks(standardize, data_in, kwargs = {\"m\":MEANS, \"s\": STDS}, template = data_in)\n",
    "data_in = data_in # this is a DataSet\n",
    "\n",
    "data_out = global_data.sel(time = times)\n",
    "data_out = data_out['Wstd'] # this is a DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074863c-e3b0-44ce-b7cb-3b5949142815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y in appropriate array shapes\n",
    "Xall = data_in\n",
    "yall = data_out\n",
    "\n",
    "levs = Xall.coords['lev'].values\n",
    "for var in SURF_VARS:\n",
    "    Xs = Xall[var].sel(lev = [71]) # 1 level above surface\n",
    "    Xsfc = Xs\n",
    "    \n",
    "    for lev in range(len(levs)-1):\n",
    "        Xsfc = xr.concat([Xsfc, Xs], dim='lev')\n",
    "        \n",
    "    Xsfc = Xsfc.assign_coords(lev=levs)\n",
    "    Xall[f\"{var}_sfc\"] = Xsfc\n",
    "\n",
    "Xall =  Xall.unify_chunks()\n",
    "Xall = Xall.to_array()\n",
    "Xall = Xall.stack( s = ('time', 'lat', 'lon', 'lev')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d08c1-9623-493d-abec-a05fcbc1d5e6",
   "metadata": {},
   "source": [
    "## Saving the Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c133e7-5571-436f-8a9b-a28418e89526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train-test split\n",
    "indices = np.arange(len(Xall.s))\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5e537-a7b7-446c-a8d2-d6c3af28f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_split(filter_indices, filter_type, Xall, yall):\n",
    "    print(\"Setting up Xall\")\n",
    "    Xall = Xall.rename({\"variable\":\"ft\"})                       \n",
    "    Xall = Xall.squeeze()\n",
    "    Xall = Xall.transpose()\n",
    "    Xall = Xall.isel(s=filter_indices)\n",
    "    Xall = Xall.chunk({\"ft\":14, \"s\": BATCH_SIZE})\n",
    "    \n",
    "    print(\"Setting up yall\")\n",
    "    yall = yall.stack(s = ('time', 'lat', 'lon', 'lev' ))\n",
    "    yall = yall.squeeze()\n",
    "    yall = yall.isel(s=filter_indices)\n",
    "    yall = yall.transpose()   \n",
    "    yall = yall.chunk({\"s\": BATCH_SIZE})\n",
    "    \n",
    "    print(\"saving Xall\")\n",
    "    Xall = Xall.reset_index('s')\n",
    "    Xall.to_netcdf(f\"X_{filter_type}.nc\")\n",
    "    \n",
    "    print(\"saving yall\")\n",
    "    yall = yall.reset_index('s')\n",
    "    yall.to_netcdff(f\"y_{filter_type}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f4c3a-c1d8-4696-95c9-47331ab22b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_split(train_indices, \"train\", Xall, yall)\n",
    "save_split(val_indices, \"val\", Xall, yall)\n",
    "save_split(test_indices, \"test\", Xall, yall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749fde0-3fe1-4d79-8119-549fa069631f",
   "metadata": {},
   "source": [
    "# Retrieving new training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce639b-d85a-4c93-8244-5d20e44a2a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"\" # INSERT LOCATION OF TRAIN DATA FROM PREVIOUS SPLIT\n",
    "X_train = xr.open_dataset(f'{file_path}/X_train.nc')\n",
    "y_train = xr.open_dataset(f'{file_path}/y_train.nc')\n",
    "\n",
    "for seed in range(15):\n",
    "    np.random.seed(seed)\n",
    "    dim_size = len(X_train.s)\n",
    "    random_idx = np.random.choice(dim_size, size=dim_size, replace=True)\n",
    "\n",
    "    Xall = X_train.isel(s = random_idx)\n",
    "    Xall.to_netcdf(f\"X_train_{seed}.nc\")\n",
    "\n",
    "    yall = y_train.isel(s = random_idx)\n",
    "    yall.to_netcdf(f\"y_train_{seed}.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Goddard)",
   "language": "python",
   "name": "goddard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
